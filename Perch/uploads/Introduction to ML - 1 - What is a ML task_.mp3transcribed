Welcome, everyone, to this first video of the semester. We'll be talking about, what is machine learning? And even though this may not be an easy question to answer, if you ask researchers working on machine learning, they might not be able to give you the same definition. What we'll do is we'll go over several problems that are typically solved with machine learning approaches, and that will give you a good idea of the different techniques that we will cover during the semester. Before we get started, I'd like to acknowledge that some of the material used in this course was adapted from similar courses that were previously taught. In particular, I adapted some of the material from CSC 321, taught by Professor Gross and Coursera's machine learning course from Professor Andrew Engine. So the first thing that I'd like to discuss and explain is what is the difference between a machine learning program and a traditional computer program? If we think about it, it boils down to how we think about writing programs and writing algorithms. So, typically, what happens if you're building a computer system is that you're going to write algorithms that solve specific problems. So let's say, for instance, you want to play the game of chess. You're going to write algorithms that you think are good strategies for winning at the game of chess. And so this will mean that you'll take certain inputs for your algorithm. This may be, for instance, the current layout of the board. And then you'll produce an output, which is the action that you should take to move a chess piece across the board. And so to come up with this output, y you're going to have to write an algorithm that is specifically designed to sort of implement a strategy for playing the game of chess. So this is usually how we write programs, and you've already done that a lot in your classes so far. Instead, when we're thinking about machining, we're going to write a single algorithm that is able to learn from data. So the big difference here is that instead of writing a program for every single task that we want to solve, we're going to write a single program that can then be applied to solve different tasks. And so this is what you have on the right here, where we're defining a learning algorithm that is going to take some data, and this algorithm is going to be able to be applied to solve specific problems afterwards. So, for instance, we may define a learning algorithm that is able to learn how to play video games in general, and then we'll be able to show it examples of different chess games. So we could show it thousands of examples of chess games, and it will be able to come up with its own strategy for playing chess through the observation of all of these games that we're providing it as input data, and then we can take this instantiation of the algorithm and apply it to play new games that it hasn't actually analyzed in the data that we've provided it. So, to make things concrete here, the learning algorithm is taking as its inputs x and y. So that's capital X and capital Y, where capital X is essentially a collection of inputs for the task that we would like the learning algorithm to solve. And capital Y is the corresponding outputs that we expect the task to be solved with when given these inputs in capital X. So we have essentially a lot of pairs of expected input and outputs for the tasks that we're trying to solve. And so the learning algorithm is going to analyze these examples, and it's going to produce an algorithm that we can then use to solve new examples of the task that we haven't seen yet. And so this is the last line at the right of the slide here, where what is going on is we're applying this algorithm that was produced by the learning algorithm on a new input x, and it's going to produce a new output y, which if we've done things correctly, and by things, I mean if we've done the machine learning component correctly, then this y would be the correct output for the task that we've been trying to solve here. So, to give an example, if these X and Y's were a lot of examples that basically represent chess games that we've played, so thousands of chess games that have been played. What we expect here is that as we provide a new example of a chess board configuration, the algorithm might, for instance, be able to come up with the right move to make in order to be more likely to win the game of chess. So why would we want to use machine learning? Well, one very common reason is when we're trying to solve a task that is very difficult to solve and for which we haven't been able to write a program ourselves. So we haven't been able to come up with an algorithm that solves this task. Two common examples are facial recognition and speech recognition. For both of these tasks, it's really hard if you're trying to solve them sort of manually by writing your own algorithm, it's really hard to think about what are the inputs that you should give to your algorithm? How should it represent the face of a person? How should it represent the audio that you're trying to parse. And then once you have these features, so the community has spent a lot of time trying to engineer these features. And then once you have these features, you still have to come up with an algorithm to sort of interpret until you have either the identity of the person in the case of the facial recognition problem, or you have for instance, the sentence, the text of the sentence being transcribed in a speech recognition task. And so what machine learning has done to these two areas is that it's replace these customized feature extraction algorithms and these customized algorithms for facial recognition and speech recognition with generic machine learning algorithms that are directly analyzing the images in the case of facial recognition, or the audio in the case of speech recognition, and are given the expected outputs for these images and speech samples as their target outputs. And then the machine learning algorithm is going to, on its own, figure out which are the features that are relevant to solve these two tasks. So of course, applying machine learning like this is not a silver bullet. There are still, for instance, ethical questions that are raised by applying machine learning to solve facial recognition. But in general, there has been a lot of technical problems, progress made in both of these areas by applying machine learning. Another example for why we should use machine learning is in problems where the algorithm needs to constantly update itself and adapt throughout time to a changing environment. And a good example of that is spam emails. So machine learning has been pretty successfully applied to spam detection. If you use email today, it's very rare that you have to manually go through your spam folder. And this is a very difficult problem because there is sort of a false positive issue here where you don't want to flag spam emails that are not spam because those are very important to the person receiving email in many cases. And the second difficulty is that adversaries trying to send you spam, so malicious users are constantly changing the way that they craft spam. And so you have to constantly update your approach at detecting spam. And so here using machine learning is a good idea because you, you'll be able to constantly update your model as new examples of spam and benign emails arrive. And again, in the case of filtering spam, applying machine learning is not a silver bullet. And one good example of this is called spear phishing where spam messages are crafted specifically to target an individual. An example here is given on this slide where this is an email that I received this year which evaded my spam filter and so was delivered to my inbox. And so this email appears to have been sent by Garth Gibson, who is the CEO of the Vector Institute, where I do research. And it is an example of why it's very hard to filter spam messages with machine learning. Here, the only feature that you can use to detect that this is not a legitimate email is the address of the sender, which is not the typical address that Garth Gibson would send emails from. And before we move on, I also wanted to comment, in case you were wondering what spam is called after. So the name comes from a can of food, which is not particularly good, but the name comes from this brand of food called spam. And lately, one other reason why we've been applying machine learning is when we believe that we can do a better job at solving a task using machine learning than by employing human programmers. And I think a good example of this is the game of go, where DeepMind demonstrated that they were able to train using machine learning, an agent that was able to perform better at the game of go than the best human player in the world. And so this shows that in certain settings, in certain controlled settings here, we were able to do a better job with machine learning than we were able to as humans, and in any case, as human programmers. And the reason that it's particularly impressive that DeepMind was able to come up with a machine learning algorithm that performs better than a human is that in the case of go, the game of go, the number of possibilities is extremely large, and so it's very hard for a program to explore all the possible game configurations. Instead, it has to come up with an actual strategy for how to learn how to play the game. And so this is why this was viewed as a very significant milestone in machine learning and artificial intelligence research, similar to what happened in the past when people figured out how to play the game of chess. And here again, the difference between chess and go is the number of possibilities and configurations for the game. So, let's make things more concrete and look at the different types of machine learning tasks that we may want to solve. Typically, we'll cluster different approaches to machine learning in three different categories. The first one is supervised learning, the second, reinforcement learning, and the third, unsupervised learning. The main difference between the three of them is the data itself and what the goal of the machine learning algorithm is. So in a supervised learning setting, typically the data will always be labeled. So what this means is for each example of an input to your task, you also have the corresponding output. To give an example that we've already discussed in the spam detection example, you know that you have an example of an email, but you also know whether this email is spam or benign. And so this is what having labeled data means. The goal of a machine learning algorithm is to look at a lot of examples of inputs and outputs and to be able to predict the correct output, the label for inputs for which we don't have the label. So what this means again is that if you have a lot of examples of emails along with labels, whether they are benign or spam, after you've seen enough of these examples, you're not able to take an unlabeled example of an email and predict whether it is benign or spam. In reinforcement learning, the setup is a little bit different. Instead of having labeled data, typically the machine learning algorithm will evolve in an environment. You can think of this environment, for instance, as the environment in which you would play a video game. And as the machine learning algorithm explores this environment, it's going to receive a reward signal. Again, following the example of the video game, you can think of the score that you've achieved in the video game as being your reward. And the goal of the machine learning algorithm is going to figure out what are the best sets of actions to explore this environment and then eventually maximize its reward signal. And so this is why reinforcement learning is very often illustrated in the setting of video games. Next, the third type of machine learning is called unsupervised learning. Here, the difference in terms of data is that the data is purely unlabeled. So we only have data that has not been annotated by a human. You can think of this as a clustering problem being a good example where you have a graph such as the one that I have here at the bottom of the slide, and you're trying to find the best way to group different nodes in this graph in clusters. And so you don't have any label, you don't know this is what the cluster should look like, but you're going to try and explore this data and extract some structure from it. And so the goal in unsupervised learning will vary a lot depending on the tasks, but typically you can think of it as looking for interesting patterns in the data. And so this will be one of the first class of machine learning approaches that we'll see in this course. In the next slides, I'd like to give you some more concrete examples of how machine learning is applied to solve specific problems. And to do that, I'm going to introduce a couple of standard data sets that we'll be using throughout the course and which have driven a lot of the research in this area. So, data sets are simply collections of data points very often when we think of supervised machine learning. So, this is the first of the three classes of machine learning algorithms that I discussed in the previous slide. The fruit fly is called mnist. So, this data set is a handwritten digit recognition data set, where the data was collected by looking at envelopes sent in the postal mail and extracting the digits contained in the zip code written by humans on these envelopes. And so the goal of this task is to look at an image of a digit that was written by a human and identify the digit that was contained in this image. So here, what you have as examples are a bunch of zeros, a bunch of ones, a bunch of images of twos, and so on. And so this data set is a supervised task. The inputs are the images that you're seeing on the screen, and the labels are the digit that is contained in each of these images. So, for instance, here, the input is this image, and the label is zero. The dataset itself contains 70,000 images that were labeled by humans. And we'll see that we typically split it in two sets, the training set with about 60,000 images, and the test set with the last 10,000. The reason that we split this data set in two training and test sets is fundamental to how we think about machine learning. And I will explain this more in details in the first lectures on supervised learning. What you should take away from this data set is that it's now a mostly solved data set in terms of machine learning vision, and we can solve it with about 99% plus accuracy. And this has been possible for multiple years now. Despite the fact that MNIST is a very simple data set, we are still learning a lot from this data set, in particular in an area that I work a lot in, which is the security and privacy of machine learning. Two examples that I give on this slide on the left is an example of an adversarial example, where the idea is that we've modified the image of a seven that was originally correctly classified as a seven by the model, and now the model will confidently classify, will predict that this image contains a three. And so we haven't been able to train machine learning models that are robust to these adversarial examples. So this is something we'll come back later in the course. Another example is the case of privacy, where we are still learning from MNIST how we can build machine learning models that do not memorize any sensitive information contained in their training set. And this is still going on today. This research is still still very active and is a very difficult problem. Another key dataset is called the imagenet dataset. It contains thousands of labels. So whereas the previous data set, MNISt only contained ten different labels corresponding to each digit, here the dataset contains thousands of categories that correspond to specific objects or animals. So here you have a couple of examples on the slide, and the input is still an image, but this time it is a color image. So it has a lot higher dimensionality. There are a lot more pixels, and there are a lot of variability in the viewpoint of the object or the animal, the lighting, and so on. Because of that, performance on this data set is not measured as achieving the exact correct prediction, but it's measured as what we call top five accuracy, which means that the correct prediction is contained in the five predictions that the model makes with the most confidence. And the reason that imagenet has been fundamental to the field of machine learning is that it sort of revived interest in a class of approaches to machine learning called neural networks. And it started what is now known as deep learning. In 2012 to 2013, researchers from the University of Toronto in Jeff Hinton's lab showed that they were able to slash the error rate drastically in the competition that was held every year on imagenet. And this revived interest in the class of approaches that rely on neural networks. So we'll see later in the course how we can train neural networks to solve vision problems using techniques that were developed at that time. In certain cases, the supervised learning tasks can span across multiple modalities. So here I've given an example where we're looking at both image and text data. The idea is to take input images and for the output to not be a label of what is in this image, but instead to be a description of the image. And so this is called caption generation. The idea is to train a machine learning model to analyze the image and then produce a text sentence that describes what is contained in the image. And so here you can see on the left, the input, the image, and the top five sentences generated by the machine learning model that we're looking at here. So, if you remember, the second type of machine learning approach that I described was reinforcement learning. Here the idea is to have an agent that interacts with an environment. So if we take the example of video games that I gave before, you could, for instance, say that you're trying to play an Atari game. And so in each time step, the agent receives observations. So these can be, for instance, the pixels that describe what the screen of the video game currently looks like from these observations, the agent is going to extract information about the state of the video game. So, for instance, in the Atari game of Breakout, which I'll show you in a second, this could be the positions of the ball and the paddle. The agent is then going to pick an action based on this observation, for instance, pressing different keys on the keyboard, which are going to modify the state. And so, because the agent is periodically receiving a reward, which is basically the number of points that it has achieved in the video game, for instance, it will be able to learn a policy that maps these observations to specific actions. And the goal of this policy will be to maximize the overall reward over time. So what you see here is a machine learning agent, trained by DeepMind, that is able to play the video game of breakout very successfully and moves the paddle in a way that makes sure it never loses the ball. If you recall, the third type of machine learning approaches that I discussed previously is unsupervised learning. And here I'll give you an example called generative modeling that has received a lot of attention from the research community lately. The idea is that we want to learn the distribution of a data set. In some sense. What that means is we want to know what an image is made up of, if we're thinking about an image data set. And the way that we evaluate how good these generative models are, is by looking at the images that we obtain if we sample from these generative models, and how these images are perceived by humans. So here on the bottom of the screen, I give you three examples of images generated by generative models in 2014. So you can see examples of deers here, dogs here, and frogs. So these images were considered impressive at that time, and as we made a lot of progress with generative modeling, and in particular an approach called generative adversarial networks, the images that we are able to sample from modern generative architectures are a lot more impressive. So here, a common example is to generate faces of humans. And some of these humans have never existed because the model is basically synthesizing new faces. You can check this website. For instance, thisperson does notexist.com, which has some examples that are generated by generative models. And every time you refresh the page, it will give you a new image. So, before we conclude this video, I wanted to give you an overview of what a typical machine learning pipeline will look like. The first step is to figure out how we are going to represent the inputs that we provide our model with. And so we'll spend some time discussing what it means to extract features from the data and how much human work is involved in this step. Once we have a way to represent our data in a format that can be parsed by a model, we'll have to choose a class of models. So that means we have to choose an hypothesis for a model. For instance, if we wanted to train a linear model, and this is the first type of model we'll see for supervised learning, what will happen is that we'll assume that our model multiplies the inputs by weight, which I wrote w here, and then adds a bias b. And so this linear model is one way to model the data data. We could also use something like a neural network. And so this is the second thing that we have to choose. Once we have chosen the model hypothesis class, we're going to train the algorithm. What this means is we're going to find the parameters of our model, what are the best values of these parameters, in order to produce the best possible predictions on our training set. And so here in the case of the linear model, the parameters would be w and b, the weight and the bias. But in the case of a neural network, you might have millions or billions of these parameters that you have to learn from. The training set is going to vary across different tasks. If you remember the class of problems that I presented around vision, for instance, if you're thinking about object recognition, you will have a data set that contains a lot of example images along with the labels, and that will be your training set, from which you will learn the parameter values. Once that is done, the last step of the pipeline is where you test the model. So you're going to give it inputs, look at the prediction that the model is making, and measure how accurate these predictions are. So, before we conclude this video, I wanted to take the time to address the question of why you should be taking this course. I think the main reason is that debugging learning algorithms requires that you understand in detail how they work. And to give you an example, there are a lot of frameworks online, a lot of software that you can download that will allow you to train very complex machine learning algorithms, things like deep neural networks, without really understanding how they work. The problem is, if you try to apply them to new, new tasks, new domains, you'll have to debug how these machine learning algorithms work. And that will be very difficult if you don't understand the principles that underlie them. And this is also why in this class, we'll will be deriving a lot of things by hand and looking at the mathematical foundations of machine learning so that you are better equipped to use machine learning in the real world.